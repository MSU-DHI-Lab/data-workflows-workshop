{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lab 01: Profile the Dataset\n\n",
        "Goal: build professional curiosity about the dataset before writing checks. You'll look at shape, missingness, and unique values.\n"
      ]
    },
	    {
	      "cell_type": "markdown",
	      "metadata": {},
	      "source": [
	        "## 1) Load the CSV\n",
	        "- Do: Load `inputs/collection_cleaned.csv` and run.\n",
	        "- Why: Profiling starts with a clean load so you trust the frame you're inspecting.\n",
	        "- You should see: A dataframe preview with 4 rows and 6 columns.\n",
	        "- If it doesn't look right: Confirm the file is CSV with headers; check the path; rerun this cell.\n"
	      ]
	    },
	    {
	      "cell_type": "code",
	      "execution_count": null,
	      "metadata": {},
	      "outputs": [],
	      "source": [
	        "import sys\nfrom pathlib import Path\nimport subprocess\n\nimport pandas as pd\n\n\ndef _find_repo_root(start: Path) -> Path:\n    for candidate in [start] + list(start.parents):\n        if (candidate / 'WORKSHOP_OVERVIEW.md').exists():\n            return candidate\n    return start\n\n\ndef _ensure_repo_root() -> Path:\n    # Colab opens notebooks without the repo files present. Clone so relative lab inputs exist.\n    if 'google.colab' in sys.modules:\n        repo_root = Path('/content/data-workflows-workshop')\n        if not repo_root.exists():\n            subprocess.run(\n                [\n                    'git',\n                    'clone',\n                    '--depth',\n                    '1',\n                    'https://github.com/MSU-DHI-Lab/data-workflows-workshop.git',\n                    str(repo_root),\n                ],\n                check=True,\n            )\n        return repo_root\n\n    return _find_repo_root(Path.cwd().resolve())\n\n\nREPO_ROOT = _ensure_repo_root()\nLAB01_ROOT = REPO_ROOT / 'day-03-quality-gates-and-reuse/01-labs/lab-01'\n\ncsv_path = LAB01_ROOT / 'inputs/collection_cleaned.csv'\ndf = pd.read_csv(csv_path)\ndf.head()\n"
	      ]
	    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Check shape and dtypes\n",
        "- Do: Run to see row/column counts and data types.\n",
        "- Why: Confirms the expected structure before writing validation.\n",
        "- You should see: (4, 6) and types showing strings for text fields, int for date if parsed.\n",
        "- If it doesn't look right: Check for extra header rows; ensure date column is numeric or castable.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.shape, df.dtypes\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) Missingness scan\n",
        "- Do: Run to count missing values per column.\n",
        "- Why: Guides which checks to add (e.g., id cannot be missing).\n",
        "- You should see: Zeros across key fields in this sample.\n",
        "- If it doesn't look right: Inspect columns with missing values; confirm they are expected.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.isna().sum()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) Unique values for rights and place\n",
        "- Do: Run to see allowed tokens and place variants.\n",
        "- Why: Informs allowed lists and normalization expectations for validation.\n",
        "- You should see: Rights tokens ['CC BY 4.0', 'Public Domain', 'Rights Reserved']; places ['Albany', 'New York City'].\n",
        "- If it doesn't look right: Check for trailing spaces; adjust allowed lists later in validation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df['rights'].unique(), df['place'].unique()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
