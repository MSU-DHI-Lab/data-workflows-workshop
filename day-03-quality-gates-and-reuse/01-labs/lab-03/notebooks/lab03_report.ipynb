{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lab 03: Generate Quality Report with Known Failures\n\n",
        "Goal: run validation on an intentionally flawed dataset, read the failures, and produce a markdown report to share.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Install Pandera (if not already)\n",
        "- Do: Run the install.\n",
        "- Why: Ensure the library is available for this session.\n",
        "- You should see: Install success.\n",
        "- If it doesn't look right: Rerun; confirm runtime has internet.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip -q install pandera[pandas]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Load schema and bad data\n",
        "- Do: Add the artifacts path, import the schema, and load `../inputs/collection_with_failures.csv`.\n",
        "- Why: Use the same rules on a dataset with known issues to see how failures surface.\n",
        "- You should see: Dataframe preview with some suspect values.\n",
        "- If it doesn't look right: Check paths; ensure the artifacts folder is accessible.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\nsys.path.append('../lab-02/artifacts')\n\nimport pandas as pd\nimport importlib\n\nschema_module = importlib.import_module('validation_schema')\nschema = schema_module.schema\n\ndf_bad = pd.read_csv('../inputs/collection_with_failures.csv')\ndf_bad.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) Run validation and capture errors\n",
        "- Do: Validate the bad dataframe in a try/except and collect the errors.\n",
        "- Why: We expect failures; capturing them lets us report clearly.\n",
        "- You should see: A `SchemaErrors` message with details on offending rows.\n",
        "- If it doesn't look right: Ensure the schema import succeeded; check that the CSV has the expected columns.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandera as pa\nfrom pandera.errors import SchemaErrors\n\ntry:\n    schema.validate(df_bad, lazy=True)\n    validation_errors = None\nexcept SchemaErrors as err:\n    validation_errors = err.failure_cases\n    display(err.failure_cases)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) Generate a markdown report\n",
        "- Do: Build a short report with counts and examples of failures.\n",
        "- Why: Reports are artifacts for stakeholders, not only programmers; they tell the story of what failed and why.\n",
        "- You should see: A markdown string with sections you can save.\n",
        "- If it doesn't look right: Check that `validation_errors` is populated; ensure lazy=True was set to collect all failures.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "total_rows = len(df_bad)\n",
        "fail_count = len(validation_errors) if validation_errors is not None else 0\n",
        "report_lines = [\n",
        "    '# Quality Report',\n",
        "    f'- Total rows: {total_rows}',\n",
        "    f'- Failed checks: {fail_count}',\n",
        "]\n",
        "if validation_errors is not None:\n",
        "    sample = validation_errors.head(10)\n",
        "    report_lines.append('## Sample failures (first 10)')\n",
        "    for _, row in sample.iterrows():\n",
        "        report_lines.append('- Column: {col} | Check: {chk} | Failure: {fail} | Index: {idx}'.format(col=row['column'], chk=row['check'], fail=row['failure_case'], idx=row['index']))\n",
        "report = '\\n'.join(report_lines)\n",
        "print(report)\n",
        "with open('validation_report.md','w') as f:\n",
        "    f.write(report)\n",
        "print('Saved report to validation_report.md')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) Reflect\n",
        "- Do: Note which checks caught issues and why they matter.\n",
        "- Why: Helps you decide whether to fix data, adjust checks, or quarantine records.\n",
        "- You should see: Your own notes summarizing next actions.\n",
        "- If it doesn't look right: Review the failure cases table; tie each check to the problem it prevented.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}